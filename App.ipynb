{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e69d3ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:45:26.064540Z",
     "start_time": "2023-07-23T16:45:26.061270Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference/tree/main/data\n",
    "# https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bdc01a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:45:27.474611Z",
     "start_time": "2023-07-23T16:45:26.631598Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0372b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:45:30.179640Z",
     "start_time": "2023-07-23T16:45:27.475862Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load PDF file from data path\n",
    "loader = DirectoryLoader('data/',\n",
    "                         glob=\"*.pdf\",\n",
    "                         loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe3dd558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:45:30.226940Z",
     "start_time": "2023-07-23T16:45:30.181302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split text from PDF into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                               chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ae3ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:46:04.103922Z",
     "start_time": "2023-07-23T16:45:30.227960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reza/.pyenv/versions/3.10.0/envs/paper-chat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                                   model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# Build and persist FAISS vector store\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "vectorstore.save_local('vectorstore/db_faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1508bbe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:46:04.107503Z",
     "start_time": "2023-07-23T16:46:04.105806Z"
    }
   },
   "outputs": [],
   "source": [
    "# qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "# Context: {context}\n",
    "# Question: {question}\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc81ce0-b78e-4eab-a1e9-1ecf16523d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_template = \"\"\"\n",
    "# You're helping a researcher to understand paper.\n",
    "# Use the following pieces of information to answer the user's question.\n",
    "# Context: {context}\n",
    "# Question: {question}\n",
    "\n",
    "# Pay attention if the context given is not answering or not convicing, you can give helpful answer that come from paper based on question. \n",
    "# Before you return helpful answer, you must mention paper title you used when you didn't use context. \n",
    "# Pay attention to not include citation number in your helpful answer.\n",
    "\n",
    "# Pay attention to keep your answer short but clear.\n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "# Only return the helpful answer below and nothing else.\n",
    "# Helpful answer:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1428ce5-b6bb-4b81-828a-b4fdfc81e4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:46:46.952522Z",
     "start_time": "2023-07-23T16:46:46.938436Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_template = \"\"\"\n",
    "You're helping machine learning researcher to understand paper. \\\n",
    "Use the following pieces of information to answer the user's question. \\\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Please pay attention to the context and the question provided. \\\n",
    "If the given context is helpful, please provide the answer based on that context. \\\n",
    "If the context is not sufficient to answer the question or is not convincing enough, \\\n",
    "you must mention the paper title and then provide a helpful answer. \n",
    "Make sure not to include citation numbers such as [15] or [54] in your response.\n",
    "\n",
    "Please pay attention to keeping your answer short but clear. \\\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \\\n",
    "Only provide the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# Config: https://github.com/marella/ctransformers#config\n",
    "# Download model from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main\n",
    "\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "# Local CTransformers wrapper for Llama-2-7B-Chat\n",
    "llm = CTransformers(model='models/llama-2-7b-chat.ggmlv3.q8_0.bin', # Location of downloaded GGML model\n",
    "                    model_type='llama', # Model type Llama\n",
    "                    config={'max_new_tokens': 1000,\n",
    "                            'temperature': 0.01,\"repetition_penalty\":2})\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Wrap prompt template in a PromptTemplate object\n",
    "def set_qa_prompt():\n",
    "    prompt = PromptTemplate(template=qa_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Build RetrievalQA object\n",
    "def build_retrieval_qa(llm, prompt, vectordb):\n",
    "    dbqa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k':2}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt})\n",
    "    return dbqa\n",
    "\n",
    "\n",
    "# Instantiate QA object\n",
    "def setup_dbqa():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                       model_kwargs={'device': 'cpu'})\n",
    "    vectordb = FAISS.load_local('vectorstore/db_faiss', embeddings)\n",
    "    qa_prompt = set_qa_prompt()\n",
    "    dbqa = build_retrieval_qa(llm, qa_prompt, vectordb)\n",
    "\n",
    "    return dbqa\n",
    "\n",
    "dbqa = setup_dbqa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b718dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:47:22.166885Z",
     "start_time": "2023-07-23T16:47:07.416753Z"
    }
   },
   "outputs": [],
   "source": [
    "response = dbqa({'query': \"What is positional encoding?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e725416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:47:22.176049Z",
     "start_time": "2023-07-23T16:47:22.173034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Positional encoding is a technique used in machine learning models such as encoder-decoders (like those described by [38]) or transformer architectures like BERT ([2]), where each position of input sequence has an associated unique fixed length vector, called the \"position embedding\". This allows model to differentiate between different positions within a sequential data.\n",
      "In this context it is used in both encoder and decoders stacks with Pdrop=0:1 which means that dropout rate for all positional embeddings are set equal 0 or not dropped at any layer of the network, so they can be computed during training process as well\n",
      "==================================================\n",
      "\n",
      "Source Document 1\n",
      "\n",
      "Source Text: positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0:1.\n",
      "7\n",
      "Document Name: data/Attention Is All You Need.pdf\n",
      "Page Number: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Source Document 2\n",
      "\n",
      "Source Text: position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "\u000fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "Document Name: data/Attention Is All You Need.pdf\n",
      "Page Number: 4\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAnswer: {response[\"result\"]}')\n",
    "print('='*50) # Formatting separator\n",
    "source_docs = response['source_documents']\n",
    "for i, doc in enumerate(source_docs):\n",
    "    print(f'\\nSource Document {i+1}\\n')\n",
    "    print(f'Source Text: {doc.page_content}')\n",
    "    print(f'Document Name: {doc.metadata[\"source\"]}')\n",
    "    print(f'Page Number: {doc.metadata[\"page\"]}\\n')\n",
    "    print('='* 50) # Formatting separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271af58-0f03-4586-85dc-19adbddaba38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a851ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='TPU-now-offers-preemptible-pricing-and-global-\\navailability.html', metadata={'source': 'data/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'page': 12}),\n",
       " Document(page_content='that contain at least one of the provided possible answers.System Dev Test\\nESIM+GloVe 51.9 52.7\\nESIM+ELMo 59.1 59.2\\nOpenAI GPT - 78.0\\nBERT BASE 81.6 -\\nBERT LARGE 86.6 86.3\\nHuman (expert)y- 85.0\\nHuman (5 annotations)y- 88.0\\nTable 4: SWAG Dev and Test accuracies.yHuman per-\\nformance is measured with 100 samples, as reported in\\nthe SWAG paper.\\n^si;j=maxj\\x15iS\\x01Ti+E\\x01Tj. We predict a non-null\\nanswer when ^si;j> s null+\\x1c, where the thresh-\\nold\\x1cis selected on the dev set to maximize F1.', metadata={'source': 'data/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'page': 6})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f12225-c9e9-4eec-b96c-fd822357937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: In Vaswani et al.'s paper (2017), a patch is defined as \"a small fixed-size subregion of interest within each image.\" Specifically in Vision Transformer, they use 8x4 non overlappingpatches for the input images and apply them through linear embeddings to create an output sequence.\n",
      "==================================================\n",
      "\n",
      "Source Document 1\n",
      "\n",
      "Source Text: scribed in Vaswani et al. (2017) and released in\n",
      "thetensor2tensor library.1Because the use\n",
      "of Transformers has become common and our im-\n",
      "plementation is almost identical to the original,\n",
      "we will omit an exhaustive background descrip-\n",
      "tion of the model architecture and refer readers to\n",
      "Vaswani et al. (2017) as well as excellent guides\n",
      "such as “The Annotated Transformer.”2\n",
      "In this work, we denote the number of layers\n",
      "(i.e., Transformer blocks) as L, the hidden size as\n",
      "Document Name: data/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\n",
      "Page Number: 2\n",
      "\n",
      "==================================================\n",
      "\n",
      "Source Document 2\n",
      "\n",
      "Source Text: the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "Document Name: data/Attention Is All You Need.pdf\n",
      "Page Number: 0\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "response = dbqa({'query': \"What is definition of patch in Vision Transformer?\"})\n",
    "print(f'\\nAnswer: {response[\"result\"]}')\n",
    "print('='*50) # Formatting separator\n",
    "source_docs = response['source_documents']\n",
    "for i, doc in enumerate(source_docs):\n",
    "    print(f'\\nSource Document {i+1}\\n')\n",
    "    print(f'Source Text: {doc.page_content}')\n",
    "    print(f'Document Name: {doc.metadata[\"source\"]}')\n",
    "    print(f'Page Number: {doc.metadata[\"page\"]}\\n')\n",
    "    print('='* 50) # Formatting separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1bab4-4fe8-49bd-9a2a-d0645a65e94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a6c0de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:47:34.292742Z",
     "start_time": "2023-07-23T16:47:22.177863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The minimum guarantee is not mentioned in any of these papers or tables provided as context for this question; therefore I cannot give a specific amount payable by adidas without additional information that may be outside my knowledge cutoff date (2019-Aug).\n",
      "==================================================\n",
      "\n",
      "Source Document 1\n",
      "\n",
      "Source Text: TPU-now-offers-preemptible-pricing-and-global-\n",
      "availability.html\n",
      "Document Name: data/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\n",
      "Page Number: 12\n",
      "\n",
      "==================================================\n",
      "\n",
      "Source Document 2\n",
      "\n",
      "Source Text: that contain at least one of the provided possible answers.System Dev Test\n",
      "ESIM+GloVe 51.9 52.7\n",
      "ESIM+ELMo 59.1 59.2\n",
      "OpenAI GPT - 78.0\n",
      "BERT BASE 81.6 -\n",
      "BERT LARGE 86.6 86.3\n",
      "Human (expert)y- 85.0\n",
      "Human (5 annotations)y- 88.0\n",
      "Table 4: SWAG Dev and Test accuracies.yHuman per-\n",
      "formance is measured with 100 samples, as reported in\n",
      "the SWAG paper.\n",
      "^si;j=maxj\u0015iS\u0001Ti+E\u0001Tj. We predict a non-null\n",
      "answer when ^si;j> s null+\u001c, where the thresh-\n",
      "old\u001cis selected on the dev set to maximize F1.\n",
      "Document Name: data/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\n",
      "Page Number: 6\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "response = dbqa({'query': \"How much is the minimum guarantee payable by adidas?\"})\n",
    "\n",
    "print(f'\\nAnswer: {response[\"result\"]}')\n",
    "print('='*50) # Formatting separator\n",
    "\n",
    "source_docs = response['source_documents']\n",
    "for i, doc in enumerate(source_docs):\n",
    "    print(f'\\nSource Document {i+1}\\n')\n",
    "    print(f'Source Text: {doc.page_content}')\n",
    "    print(f'Document Name: {doc.metadata[\"source\"]}')\n",
    "    print(f'Page Number: {doc.metadata[\"page\"]}\\n')\n",
    "    print('='* 50) # Formatting separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadd627-442e-40af-ae44-ed6de8140875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89d830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7d1057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-23T16:48:13.733194Z",
     "start_time": "2023-07-23T16:47:57.284009Z"
    }
   },
   "outputs": [],
   "source": [
    "# response = dbqa({'query': \"What is embeddings?\"})\n",
    "# print(f'\\nAnswer: {response[\"result\"]}')\n",
    "# print('='*50) # Formatting separator\n",
    "\n",
    "# source_docs = response['source_documents']\n",
    "# for i, doc in enumerate(source_docs):\n",
    "#     print(f'\\nSource Document {i+1}\\n')\n",
    "#     print(f'Source Text: {doc.page_content}')\n",
    "#     print(f'Document Name: {doc.metadata[\"source\"]}')\n",
    "#     print(f'Page Number: {doc.metadata[\"page\"]}\\n')\n",
    "#     print('='* 50) # Formatting separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18b217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd4aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a644347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
